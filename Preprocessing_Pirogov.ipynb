{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surrounded-camcorder",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "biological-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import torchio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premier-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "\n",
    "import utils.data_processor as data_processor\n",
    "imp.reload(data_processor)\n",
    "from utils.data_processor import *\n",
    "\n",
    "import utils.visualization_tools as visualization_tools\n",
    "imp.reload(visualization_tools)\n",
    "from utils.visualization_tools import *\n",
    "\n",
    "import utils.routine_pirogov as routine\n",
    "imp.reload(routine)\n",
    "from utils.routine_pirogov import *\n",
    "\n",
    "import utils.metrics as metrics\n",
    "imp.reload(metrics)\n",
    "from utils.metrics import *\n",
    "\n",
    "import utils.metrics_deep_mind as metrics_deep_mind\n",
    "imp.reload(metrics_deep_mind)\n",
    "from utils.metrics_deep_mind import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "virgin-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brain_borders(brain):\n",
    "    \n",
    "    '''\n",
    "    Function finds borders of non-zero pixels for a given brain.\n",
    "\n",
    "    Arguments:\n",
    "        * brain (np.array): brain\n",
    "        \n",
    "    Output:\n",
    "        * [(min(axial_coord), max(axial_coord)),\n",
    "        (min(coronal_coord), max(coronal_coord)),\n",
    "        (min(sagital_coord), max(sagital_coord))] : corresponding borders \n",
    "    '''\n",
    "    \n",
    "    if isinstance(img, torch.Tensor):\n",
    "        brain = brain.numpy()\n",
    "        if (len(brain.shape) > 3):\n",
    "            brain = brain[0,:,:,:]\n",
    "                \n",
    "    elif isinstance(brain, nibabel.nifti1.Nifti1Image):    \n",
    "        brain = brain.get_fdata()\n",
    "    \n",
    "    axial_coord = []\n",
    "    for i in range(brain.shape[2]):\n",
    "        if (brain[:, :, i] != 0).any():\n",
    "            axial_coord.append(i)\n",
    "    \n",
    "    coronal_coord = []\n",
    "    for i in range(brain.shape[1]):\n",
    "        if (brain[:, i, :] != 0).any():\n",
    "            coronal_coord.append(i)\n",
    "    \n",
    "    sagital_coord = []\n",
    "    for i in range(brain.shape[0]):\n",
    "        if (brain[i, :, :] != 0).any():\n",
    "            sagital_coord.append(i)\n",
    "                        \n",
    "    return [(min(axial_coord), max(axial_coord)), (min(coronal_coord), max(coronal_coord)), (min(sagital_coord), max(sagital_coord))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_borders(dataset_path = '../datasets/new_dataset/fcd_brains/'):\n",
    "    \n",
    "    '''\n",
    "    Function finds borders of non-zero pixels for dataset of brains.\n",
    "\n",
    "    Arguments:\n",
    "        * dataset_path (str): path to dataset\n",
    "    '''\n",
    "    \n",
    "    sagital_min, sagital_max, coronal_min, coronal_max, axial_min, axial_max = [], [], [], [], [], []\n",
    "\n",
    "    for brain in tqdm(os.listdir(dataset_path)):\n",
    "        coords =  get_brain_borders(load_nii_to_array(dataset_path + brain))\n",
    "        sagital_min.append(coords[0][0])\n",
    "        sagital_max.append(coords[0][1])\n",
    "        coronal_min.append(coords[1][0])\n",
    "        coronal_max.append(coords[1][1])\n",
    "        axial_min.append(coords[2][0])\n",
    "        axial_max.append(coords[2][1])\n",
    "        \n",
    "    return np.min(sagital_min), np.max(sagital_max), np.min(coronal_min), np.max(coronal_max), np.min(axial_min), np.max(axial_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-style",
   "metadata": {},
   "source": [
    "# Crope brains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "competent-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crope_brains(dataset_path='../datasets/new_dataset/fcd_brains/',\n",
    "                 new_dataset_path='../datasets/croped_new_dataset/fcd_brains/'):\n",
    "    \n",
    "    '''\n",
    "    Function makes croped datset, based on results of get_dataset_borders function\n",
    "\n",
    "    Arguments:\n",
    "        * dataset_path (str): path to datset\n",
    "        * new_dataset_path (str): path to croped dataset\n",
    "    '''\n",
    "    \n",
    "    sagital_min, sagital_max, coronal_min, coronal_max, axial_min, axial_max = get_dataset_borders(dataset_path)\n",
    "    for brain in tqdm(os.listdir(dataset_path)):\n",
    "        \n",
    "        array_brain = load_nii_to_array(dataset_path + '/' + brain)\n",
    "        array_brain = array_brain[sagital_min: sagital_max, coronal_min: coronal_max, axial_min: axial_max]\n",
    "\n",
    "        affine = nib.load(dataset_path + '/' + brain).affine\n",
    "        new_image = nib.Nifti1Image(array_brain.astype(float), affine)\n",
    "        \n",
    "        nib.save(new_image, new_dataset_path + brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "medium-effects",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [03:05<00:00,  1.52s/it]\n",
      "100%|██████████| 122/122 [07:51<00:00,  3.87s/it]\n"
     ]
    }
   ],
   "source": [
    "crope_brains(dataset_path='../datasets/new_dataset/fcd_brains/',\n",
    "             new_dataset_path='../datasets/croped_new_dataset/fcd_brains/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-federation",
   "metadata": {},
   "source": [
    "# Crope masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conceptual-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crope_masks(mask_path = '../datasets/new_dataset/masks/',\n",
    "                dataset_path = '../datasets/new_dataset/fcd_brains/',\n",
    "                new_mask_path = '../datasets/croped_new_dataset/masks/',\n",
    "                new_datset_path = '../datasets/croped_new_dataset/fcd_brains/'):\n",
    "    '''\n",
    "    Function makes croped masks, based on results of get_dataset_borders\n",
    "\n",
    "    Arguments:\n",
    "        * folder_path (str): path to folder with .nii.gz/.nii files \n",
    "        * new_folder_path (str): path to new folder \n",
    "    '''\n",
    "    \n",
    "    sagital_min, sagital_max, coronal_min, coronal_max, axial_min, axial_max = get_dataset_borders(dataset_path)\n",
    "    for mask in tqdm(os.listdir(mask_path)):\n",
    "        array_mask = load_nii_to_array(mask_path + mask)\n",
    "        array_mask = array_mask[sagital_min: sagital_max, coronal_min: coronal_max, axial_min: axial_max]\n",
    "        array_mask[array_mask > 0] = 1.\n",
    "\n",
    "        name = mask[5:].split('.')[0]\n",
    "        affine = nib.load(new_datset_path + name + '.1.nii.gz').affine\n",
    "        new_mask = nib.Nifti1Image(array_mask.astype(float), affine)\n",
    "        \n",
    "        nib.save(new_mask, new_mask_path + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nearby-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [03:02<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "crope_masks(mask_path = '../datasets/new_dataset/masks/',\n",
    "                dataset_path = '../datasets/new_dataset/fcd_brains/',\n",
    "                new_mask_path = '../datasets/croped_new_dataset/masks/',\n",
    "                new_datset_path = '../datasets/croped_new_dataset/fcd_brains/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-colony",
   "metadata": {},
   "source": [
    "# Making CSV file with paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "organized-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(dataset_path='../datasets/croped_new_dataset/fcd_brains/',\n",
    "            mask_path='../datasets/croped_new_dataset/masks/',\n",
    "            name='Pirogov_paths.csv'):\n",
    "    \n",
    "    '''\n",
    "    Function makes csv file with all the paths \n",
    "\n",
    "    Arguments:\n",
    "        * folder_path (str): path to folder with .nii.gz/.nii files \n",
    "        * new_folder_path (str): path to new folder \n",
    "    '''\n",
    "    \n",
    "    subjects = set([x.split('.')[0] for x in os.listdir(dataset_path) if x[:3] == 'fcd'])\n",
    "    paths_to_all_files = pd.DataFrame(columns = ['Subject', 'T1', 'T2', 'T3', 'fcd_mask'])\n",
    "\n",
    "    for i, subject in enumerate(subjects):\n",
    "\n",
    "        path_to_mask = mask_path + 'mask_' + subject + '.1.nii.gz'\n",
    "        path_to_T1_file = dataset_path + subject + '.1.nii.gz'\n",
    "        path_to_T2_file = dataset_path + subject + '.2.nii.gz'\n",
    "        path_to_T3_file = dataset_path + subject + '.3.nii.gz'\n",
    "\n",
    "        paths_to_all_files.loc[i] = [subject, \n",
    "                                      path_to_T1_file, \n",
    "                                      path_to_T2_file, \n",
    "                                      path_to_T3_file,\n",
    "                                      path_to_mask]\n",
    "\n",
    "    paths_to_all_files.to_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "freelance-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_csv(dataset_path='../datasets/croped_new_dataset/fcd_brains/',\n",
    "            mask_path='../datasets/croped_new_dataset/masks/',\n",
    "            name='Pirogov_paths.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-forest",
   "metadata": {},
   "source": [
    "# Making landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "intense-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_to_all_files = pd.read_csv('Pirogov_paths.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "endangered-pastor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:51<00:00,  1.97s/it]\n",
      "100%|██████████| 26/26 [00:56<00:00,  2.18s/it]\n",
      "100%|██████████| 26/26 [00:56<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "t1_landmarks_path = 'landmarks/Pirogov_t1_landmarks.npy'\n",
    "t2_landmarks_path = 'landmarks/Pirogov_t2_landmarks.npy'\n",
    "t3_landmarks_path = 'landmarks/Pirogov_t3_landmarks.npy'\n",
    "\n",
    "t1_paths = paths_to_all_files.T1.values.tolist()\n",
    "t2_paths = paths_to_all_files.T2.values.tolist()\n",
    "t3_paths = paths_to_all_files.T3.values.tolist()\n",
    "\n",
    "t1_landmarks = (\n",
    "    #t1_landmarks_path\n",
    "    #if t1_landmarks_path.is_file()\n",
    "    #else \n",
    "    HistogramStandardization.train(t1_paths)\n",
    ")\n",
    "np.save(t1_landmarks_path, t1_landmarks, allow_pickle=True)\n",
    "\n",
    "t2_landmarks = (\n",
    "    #t2_landmarks_path\n",
    "    #if t2_landmarks_path.is_file()\n",
    "    #else\n",
    "    HistogramStandardization.train(t2_paths)\n",
    ")\n",
    "np.save(t2_landmarks_path, t2_landmarks, allow_pickle=True)\n",
    "\n",
    "t3_landmarks = (\n",
    "    #t2_landmarks_path\n",
    "    #if t2_landmarks_path.is_file()\n",
    "    #else\n",
    "    HistogramStandardization.train(t3_paths)\n",
    ")\n",
    "np.save(t3_landmarks_path, t3_landmarks, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
